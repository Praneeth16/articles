{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI Agents for NLP Data Generation\n",
    "\n",
    "This notebook demonstrates how to build a custom AI agent for generating synthetic text data to address class imbalance in NLP datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pandas scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "\n",
    "# setting up the presets\n",
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# setup OpenAI API key\n",
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating the NLP Synthetic Data Agent\n",
    "\n",
    "Base structure of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_patterns_with_llm(client: OpenAI, texts: List[str], label: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Like a student taking detailed notes, this function uses an AI to analyze\n",
    "    writing patterns in our sample texts\n",
    "    \"\"\"\n",
    "    analysis_prompt = f\"\"\"\n",
    "    Analyze the following {len(texts)} text samples for class '{label}' and identify their common patterns:\n",
    "\n",
    "    Texts:\n",
    "    {json.dumps(texts, indent=2)}\n",
    "\n",
    "    Analyze and return a JSON object with these components:\n",
    "    1. writing_style: List of identified writing styles (e.g., formal, casual, technical, emotional)\n",
    "    2. semantic_patterns: List of semantic patterns (e.g., sentiment trends, topic focus, common themes)\n",
    "    3. structural_patterns: List of sentence structure patterns (e.g., length, complexity)\n",
    "    4. context_rules: List of contextual patterns and rules (e.g., typical scenarios, common subjects)\n",
    "\n",
    "    Format your response as a JSON object with these exact keys.\n",
    "    Be specific and detailed in your analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert linguistic pattern analyzer that identifies detailed writing patterns and styles.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": analysis_prompt\n",
    "        }],\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "def analyze_class_patterns(client: OpenAI, texts: List[str], labels: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Like organizing notes by topic, this function analyzes patterns for each type of text\n",
    "    \"\"\"\n",
    "    # Figure out how many of each type we have\n",
    "    class_distribution = Counter(labels)\n",
    "    \n",
    "    # Take a small sample from each type (10% of each class)\n",
    "    samples_per_class = {}\n",
    "    for label in set(labels):\n",
    "        class_texts = [text for text, l in zip(texts, labels) if l == label]\n",
    "        sample_size = max(1, int(len(class_texts) * 0.1))\n",
    "        import random\n",
    "        samples_per_class[label] = random.sample(class_texts, sample_size)\n",
    "    \n",
    "    # Find important words for each type using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=50)\n",
    "    \n",
    "    class_keywords = {}\n",
    "    label_patterns = {}\n",
    "    \n",
    "    for label, class_texts in samples_per_class.items():\n",
    "        if class_texts:\n",
    "            # Get important words\n",
    "            tfidf_matrix = vectorizer.fit_transform(class_texts)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            class_keywords[label] = list(feature_names)\n",
    "            \n",
    "            # Get AI analysis for this type\n",
    "            llm_patterns = analyze_text_patterns_with_llm(client, class_texts, label)\n",
    "            \n",
    "            # Combine everything we learned\n",
    "            label_patterns[label] = {\n",
    "                'writing_style': llm_patterns['writing_style'],\n",
    "                'key_phrases': class_keywords[label],\n",
    "                'semantic_patterns': llm_patterns['semantic_patterns'],\n",
    "                'structural_patterns': llm_patterns['structural_patterns'],\n",
    "                'context_rules': llm_patterns['context_rules']\n",
    "            }\n",
    "    \n",
    "    return label_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Creation Phase: Implementing the Generation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_samples(client: OpenAI, patterns: Dict, target_label: str, num_samples: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Uses AI to generate new text samples based on what we learned\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a synthetic text generation expert. Generate {num_samples} new text samples \n",
    "    that match this class's patterns:\n",
    "\n",
    "    1. Writing Style:\n",
    "    {json.dumps(patterns['writing_style'], indent=2)}\n",
    "\n",
    "    2. Key Phrases:\n",
    "    {json.dumps(patterns['key_phrases'], indent=2)}\n",
    "\n",
    "    3. Semantic Patterns:\n",
    "    {json.dumps(patterns['semantic_patterns'], indent=2)}\n",
    "\n",
    "    4. Structural Patterns:\n",
    "    {json.dumps(patterns['structural_patterns'], indent=2)}\n",
    "\n",
    "    5. Context Rules:\n",
    "    {json.dumps(patterns['context_rules'], indent=2)}\n",
    "\n",
    "    Requirements:\n",
    "    - Generate completely unique and original texts\n",
    "    - Maintain the identified patterns and style\n",
    "    - Ensure natural language flow\n",
    "    - Create diverse examples while staying within the class patterns\n",
    "    - Avoid exact copying of training examples\n",
    "\n",
    "    Return a JSON array with the structure:\n",
    "        \"samples\": [\n",
    "            \"sample text 1\",\n",
    "            \"sample text 2\",\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a synthetic text generation expert that creates realistic but unique samples.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }],\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    return result.get(\"samples\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Validation Phase: Ensuring Data Integrity and Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_synthetic_sample(client: OpenAI, text: str, target_label: str, patterns: Dict) -> tuple[bool, list, list]:\n",
    "    \"\"\"\n",
    "    Checks if our generated text matches the patterns we learned\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Validate if this synthetic text matches the patterns for class '{target_label}':\n",
    "    \n",
    "    Text to validate:\n",
    "    {text}\n",
    "    \n",
    "    Expected Patterns:\n",
    "    {json.dumps(patterns, indent=2)}\n",
    "    \n",
    "    Check for:\n",
    "    1. Does it follow the identified writing style?\n",
    "    2. Does it use appropriate key phrases?\n",
    "    3. Does it match the semantic patterns?\n",
    "    4. Does it follow structural patterns?\n",
    "    5. Does it adhere to context rules?\n",
    "    \n",
    "    Return JSON with format:\n",
    "        \"is_valid\": true/false,\n",
    "        \"issues\": [\"issue1\", \"issue2\", ...],\n",
    "        \"suggestions\": [\"suggestion1\", \"suggestion2\", ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }],\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    return (\n",
    "        result.get(\"is_valid\", False),\n",
    "        result.get(\"issues\", []),\n",
    "        result.get(\"suggestions\", [])\n",
    "    )\n",
    "\n",
    "def augment_dataset(client: OpenAI, texts: List[str], labels: List[str], \n",
    "                   target_label: str, num_synthetic: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Creates a balanced dataset by adding synthetic samples for a given class\n",
    "    \"\"\"\n",
    "    # First analyze patterns in the dataset\n",
    "    patterns = analyze_class_patterns(client, texts, labels)\n",
    "    \n",
    "    # Generate synthetic samples\n",
    "    synthetic_samples = []\n",
    "    valid_count = 0\n",
    "    \n",
    "    while valid_count < num_synthetic:\n",
    "        # Generate in batches of 10\n",
    "        batch = generate_synthetic_samples(\n",
    "            client,\n",
    "            patterns[target_label], \n",
    "            target_label, \n",
    "            min(10, num_synthetic - valid_count)\n",
    "        )\n",
    "        \n",
    "        for sample in batch:\n",
    "            is_valid, issues, suggestions = validate_synthetic_sample(\n",
    "                client,\n",
    "                sample, \n",
    "                target_label,\n",
    "                patterns[target_label]\n",
    "            )\n",
    "            if is_valid:\n",
    "                synthetic_samples.append(sample)\n",
    "                valid_count += 1\n",
    "                if valid_count >= num_synthetic:\n",
    "                    break\n",
    "    \n",
    "    # Combine with original dataset\n",
    "    augmented_texts = texts + synthetic_samples\n",
    "    augmented_labels = labels + [target_label] * len(synthetic_samples)\n",
    "    \n",
    "    return augmented_texts, augmented_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Analyzing original data...\n",
      "🤖 Generating 1 new neutral reviews...\n",
      "\n",
      "Original Distribution: {'positive': 2, 'neutral': 1, 'negative': 1}\n",
      "New Distribution: {'positive': 2, 'neutral': 2, 'negative': 1}\n"
     ]
    }
   ],
   "source": [
    "#Run our complete data generation pipeline\n",
    "\n",
    "# Setup OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Sample movie reviews\n",
    "sample_reviews = [\n",
    "    \"This movie was absolutely fantastic! Great acting and wonderful plot.\",\n",
    "    \"Decent film but the pacing was a bit slow. Good performance by the lead actor.\",\n",
    "    \"Not worth the ticket price. Poor storyline and mediocre acting.\",\n",
    "    \"A masterpiece of modern cinema. Every scene was perfectly crafted.\"\n",
    "]\n",
    "\n",
    "# Sample labels\n",
    "sample_labels = [\"positive\", \"neutral\", \"negative\", \"positive\"]\n",
    "\n",
    "# Let's say we want to generate more neutral reviews to balance our dataset\n",
    "print(\"📚 Analyzing original data...\")\n",
    "\n",
    "# Find out how many samples we need\n",
    "label_counts = Counter(sample_labels)\n",
    "desired_samples = max(label_counts.values())\n",
    "target_label = \"neutral\"\n",
    "synthetic_needed = desired_samples - label_counts[target_label]\n",
    "\n",
    "print(f\"🤖 Generating {synthetic_needed} new {target_label} reviews...\")\n",
    "\n",
    "# Generate balanced dataset\n",
    "augmented_texts, augmented_labels = augment_dataset(\n",
    "    client=client,\n",
    "    texts=sample_reviews,\n",
    "    labels=sample_labels,\n",
    "    target_label=target_label,\n",
    "    num_synthetic=synthetic_needed\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal Distribution:\", dict(Counter(sample_labels)))\n",
    "print(\"New Distribution:\", dict(Counter(augmented_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented text -\n",
      " [\"The lead actor's performance was solid, but the film had some slow pacing that dragged it down a bit.\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"Augmented text -\\n\", list(set(augmented_texts) - set(sample_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
